{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.fft import fftn, ifftn\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:13:04.698916Z","iopub.execute_input":"2024-03-03T22:13:04.699821Z","iopub.status.idle":"2024-03-03T22:13:04.704721Z","shell.execute_reply.started":"2024-03-03T22:13:04.699785Z","shell.execute_reply":"2024-03-03T22:13:04.703373Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Global Filter","metadata":{}},{"cell_type":"code","source":"class GlobalFilter(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(GlobalFilter, self).__init__()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(in_channels, out_channels)\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        x = self.global_avg_pool(x).view(b, c)\n        x = self.fc(x).view(b, c, 1, 1)\n        return x\n\nin_channels = 512  # number of input channels\nout_channels = 512  # number of output channels\nglobal_filter = GlobalFilter(in_channels, out_channels)\n\n# Example input tensor\nx = torch.rand(1, in_channels, 32, 32)  # Batch size of 1, 32x32 feature map\nfiltered_x = global_filter(x)\nprint(filtered_x.shape)  \n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:18:42.188011Z","iopub.execute_input":"2024-03-03T21:18:42.188623Z","iopub.status.idle":"2024-03-03T21:18:42.208768Z","shell.execute_reply.started":"2024-03-03T21:18:42.188588Z","shell.execute_reply":"2024-03-03T21:18:42.207864Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([1, 512, 1, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Attention Filter Gate Network (AFGN)","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UpConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpConv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.up(x)\n\nclass FFTBlock(nn.Module):\n    def __init__(self):\n        super(FFTBlock, self).__init__()\n\n    def forward(self, x):\n        x_fft = torch.fft.fft2(x)\n        return x_fft\n\nclass iFFTBlock(nn.Module):\n    def __init__(self):\n        super(iFFTBlock, self).__init__()\n\n    def forward(self, x):\n        x_ifft = torch.fft.ifft2(x)\n        return x_ifft.real\n\nclass GlobalFiltersAndAFG(nn.Module):\n    # Placeholder for Global Filters and AFG operations\n    def __init__(self):\n        super(GlobalFiltersAndAFG, self).__init__()\n\n    def forward(self, x):\n        # Implement the actual operation of Global Filters and AFG\n        # This is a placeholder. Replace with actual implementation.\n        return x\n\nclass FrequencyAttentionUNet(nn.Module):\n    def __init__(self, img_ch=3, output_ch=1):\n        super(FrequencyAttentionUNet, self).__init__()\n        self.encoder1 = ConvBlock(img_ch, 64)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = ConvBlock(64, 128)\n        self.fft_block = FFTBlock()\n        self.global_filters_afg = GlobalFiltersAndAFG()\n        self.ifft_block = iFFTBlock()\n        self.upconv = UpConv(128, 64)\n        self.decoder1 = ConvBlock(128, 64)\n        self.final_conv = nn.Conv2d(64, output_ch, kernel_size=1)\n\n    def forward(self, x):\n        e1 = self.encoder1(x)\n        e2 = self.pool(e1)\n        e2 = self.encoder2(e2)\n        \n        f = self.fft_block(e2)\n        f = self.global_filters_afg(f)\n        f = self.ifft_block(f)\n        \n        d1 = self.upconv(f)\n        d1 = torch.cat((e1, d1), dim=1)\n        d1 = self.decoder1(d1)\n        out = self.final_conv(d1)\n        return out\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:13:09.430226Z","iopub.execute_input":"2024-03-03T22:13:09.430871Z","iopub.status.idle":"2024-03-03T22:13:09.450390Z","shell.execute_reply.started":"2024-03-03T22:13:09.430840Z","shell.execute_reply":"2024-03-03T22:13:09.449302Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = FrequencyAttentionUNet(img_ch=3, output_ch=1)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:13:13.958954Z","iopub.execute_input":"2024-03-03T22:13:13.959319Z","iopub.status.idle":"2024-03-03T22:13:13.973081Z","shell.execute_reply.started":"2024-03-03T22:13:13.959289Z","shell.execute_reply":"2024-03-03T22:13:13.972084Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"FrequencyAttentionUNet(\n  (encoder1): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder2): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (fft_block): FFTBlock()\n  (global_filters_afg): GlobalFiltersAndAFG()\n  (ifft_block): iFFTBlock()\n  (upconv): UpConv(\n    (up): Sequential(\n      (0): Upsample(scale_factor=2.0, mode='bilinear')\n      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): ReLU(inplace=True)\n    )\n  )\n  (decoder1): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Attention U-Net","metadata":{}},{"cell_type":"markdown","source":"### Attention Gate","metadata":{}},{"cell_type":"code","source":"class AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(F_int)\n        )\n        \n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(F_int)\n        )\n        \n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = F.relu(g1 + x1, inplace=True)\n        psi = self.psi(psi)\n        return x * psi","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:13:44.860411Z","iopub.execute_input":"2024-03-03T22:13:44.860820Z","iopub.status.idle":"2024-03-03T22:13:44.871222Z","shell.execute_reply.started":"2024-03-03T22:13:44.860787Z","shell.execute_reply":"2024-03-03T22:13:44.870202Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### U-Net Model Architecture","metadata":{}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, img_ch=3, output_ch=1):\n        super(UNet, self).__init__()\n\n        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = ConvBlock(img_ch, 64)\n        self.Conv2 = ConvBlock(64, 128)\n        self.Conv3 = ConvBlock(128, 256)\n        self.Conv4 = ConvBlock(256, 512)\n        self.Conv5 = ConvBlock(512, 1024)\n\n        self.Up5 = UpConv(1024, 512)\n        self.Att5 = AttentionGate(F_g=512, F_l=512, F_int=256)\n        self.Up_conv5 = ConvBlock(1024, 512)\n\n        self.Up4 = UpConv(512, 256)\n        self.Att4 = AttentionGate(F_g=256, F_l=256, F_int=128)\n        self.Up_conv4 = ConvBlock(512, 256)\n\n        self.Up3 = UpConv(256, 128)\n        self.Att3 = AttentionGate(F_g=128, F_l=128, F_int=64)\n        self.Up_conv3 = ConvBlock(256, 128)\n\n        self.Up2 = UpConv(128, 64)\n        self.Att2 = AttentionGate(F_g=64, F_l=64, F_int=32)\n        self.Up_conv2 = ConvBlock(128, 64)\n\n        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x5 = self.Conv5(x5)\n\n        # decoding + concat path\n        d5 = self.Up5(x5)\n        x4 = self.Att5(g=d5, x=x4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        x3 = self.Att4(g=d4, x=x3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        x2 = self.Att3(g=d3, x=x2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g=d2, x=x1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv_1x1(d2)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:13:21.412874Z","iopub.execute_input":"2024-03-03T22:13:21.413259Z","iopub.status.idle":"2024-03-03T22:13:21.435184Z","shell.execute_reply.started":"2024-03-03T22:13:21.413229Z","shell.execute_reply":"2024-03-03T22:13:21.434037Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = UNet(img_ch=3, output_ch=1)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:08:34.155261Z","iopub.execute_input":"2024-03-03T22:08:34.155942Z","iopub.status.idle":"2024-03-03T22:08:34.482339Z","shell.execute_reply.started":"2024-03-03T22:08:34.155912Z","shell.execute_reply":"2024-03-03T22:08:34.481405Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"UNet(\n  (Maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (Conv1): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv2): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv3): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv4): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv5): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up5): UpConv(\n    (up): Sequential(\n      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n    )\n  )\n  (Att5): AttentionGate(\n    (W_g): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n  )\n  (Up_conv5): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up4): UpConv(\n    (up): Sequential(\n      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n    )\n  )\n  (Att4): AttentionGate(\n    (W_g): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n  )\n  (Up_conv4): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up3): UpConv(\n    (up): Sequential(\n      (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n    )\n  )\n  (Att3): AttentionGate(\n    (W_g): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n  )\n  (Up_conv3): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Up2): UpConv(\n    (up): Sequential(\n      (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n    )\n  )\n  (Att2): AttentionGate(\n    (W_g): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (W_x): Sequential(\n      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (psi): Sequential(\n      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Sigmoid()\n    )\n  )\n  (Up_conv2): ConvBlock(\n    (conv): Sequential(\n      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (Conv_1x1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Segmentation Metric","metadata":{}},{"cell_type":"code","source":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target*inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0 \n    return intersection/union","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:04:14.528146Z","iopub.execute_input":"2024-03-03T22:04:14.528499Z","iopub.status.idle":"2024-03-03T22:04:14.533581Z","shell.execute_reply.started":"2024-03-03T22:04:14.528473Z","shell.execute_reply":"2024-03-03T22:04:14.532700Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Dice Loss Function","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:04:21.903992Z","iopub.execute_input":"2024-03-03T22:04:21.904789Z","iopub.status.idle":"2024-03-03T22:04:21.913009Z","shell.execute_reply.started":"2024-03-03T22:04:21.904750Z","shell.execute_reply":"2024-03-03T22:04:21.911890Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# check Loss Function\nDiceLoss()(torch.tensor([0.8, 1., 1.]), \n              torch.tensor([1.,0.9,1.]))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T22:05:11.258247Z","iopub.execute_input":"2024-03-03T22:05:11.258613Z","iopub.status.idle":"2024-03-03T22:05:11.267191Z","shell.execute_reply.started":"2024-03-03T22:05:11.258586Z","shell.execute_reply":"2024-03-03T22:05:11.266097Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor(0.1477)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Softmax CE Loss","metadata":{}}]}